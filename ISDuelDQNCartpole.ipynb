{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duelling DQN with Importance Sampling in the Cartpole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "EPISODE_COUNT = 10000\n",
    "EPISODE_LENGTH = 200\n",
    "EXPERIENCE_CAPACITY = 5000\n",
    "OBS_SIZE = 4\n",
    "USE_SOFT_TARGET_UPDATE = True\n",
    "TARGET_NET_REFRESH_RATE = 200\n",
    "MINIBATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelQ(nn.Module):\n",
    "    def __init__(self, action_count):\n",
    "        super(DuelQ, self).__init__()\n",
    "        self.fc1 = nn.Linear(OBS_SIZE, 64)\n",
    "        self.v1 = nn.Linear(64, 64)\n",
    "        self.v2 = nn.Linear(64, 1)\n",
    "        self.a1 = nn.Linear(64, 64)\n",
    "        self.a2 = nn.Linear(64, action_count)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_last_dim = x.dim() - 1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v_out = self.v2(F.relu(self.v1(x)))\n",
    "        a_out = self.a2(F.relu(self.a1(x)))\n",
    "        a_max = torch.max(a_out, dim=x_last_dim)[0]\n",
    "        return v_out + a_out - a_max.view(torch.numel(a_max), 1)\n",
    "\n",
    "\n",
    "def transfer_state(src, dst):\n",
    "    dst.load_state_dict(src.state_dict())\n",
    "\n",
    "\n",
    "def duel_argmax_action(net,state):\n",
    "    with torch.no_grad():\n",
    "        argmax = torch.max(net.forward(state), 1)[1].int().item()\n",
    "    return argmax\n",
    "\n",
    "\n",
    "def duel_argmax_action_batch(net, state):\n",
    "    out = net.forward(state)\n",
    "    return torch.max(out, 1)[1].unsqueeze(1).int()\n",
    "\n",
    "\n",
    "def empty_transition_block():\n",
    "    return torch.zeros(1, OBS_SIZE * 2 + 3)\n",
    "\n",
    "\n",
    "def polyak(src, dst, p):\n",
    "    for src_p, dst_p in zip(src.parameters(), dst.parameters()):\n",
    "        dst_p.data.copy_(p * dst_p.data + (1 - p) * src_p.data)\n",
    "\n",
    "\n",
    "def duel_dqn_target(target_network, batch):\n",
    "    with torch.no_grad():\n",
    "        out = target_network.forward(batch[:, OBS_SIZE + 1: 2 * OBS_SIZE + 1])\n",
    "    return torch.max(out, 1)[0].unsqueeze(1)\n",
    "\n",
    "\n",
    "def duel_double_dqn_target(target_network, q_network, batch):\n",
    "    with torch.no_grad():\n",
    "        next_s_actions = duel_argmax_action_batch(q_network, batch[:, OBS_SIZE + 1: 2 * OBS_SIZE + 1])\n",
    "        out = target_network.forward(batch[:, OBS_SIZE + 1: 2*OBS_SIZE + 1])\n",
    "    return out.gather(1, next_s_actions.long())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportanceWeighedSample(object):\n",
    "    def __init__(self, _isb, samples, weights, indices, i):\n",
    "        self.batch = samples\n",
    "        self.weights = weights\n",
    "        self.__isb = _isb\n",
    "        self.__indices = indices\n",
    "        self.i = i\n",
    "        \n",
    "    def update(self, losses):\n",
    "        assert self.__isb.i == self.i\n",
    "        self.__isb.update_priorities(losses, self.__indices)\n",
    "        self.__isb.update_beta()\n",
    "        \n",
    "\n",
    "class ImportanceSamplingBuffer(object):\n",
    "    def __init__(self, alpha, initial_beta):\n",
    "        assert 0 <= initial_beta <= 1\n",
    "        self.capacity = EXPERIENCE_CAPACITY\n",
    "        self.alpha = alpha\n",
    "        self.transitions = torch.zeros((EXPERIENCE_CAPACITY, OBS_SIZE * 2 + 3))\n",
    "        self.priorities = torch.zeros(EXPERIENCE_CAPACITY)\n",
    "        self.initial_max_priority = 1.0\n",
    "        self.size = 0\n",
    "        self.i = 0\n",
    "        self.initial_beta = initial_beta\n",
    "        self.beta = initial_beta\n",
    "        self.beta_i = 0\n",
    "        self.beta_i_at_max = int(EPISODE_COUNT*EPISODE_LENGTH/10)\n",
    "    \n",
    "    def push(self, transition):\n",
    "        self.transitions[self.i] = transition\n",
    "        max_priority = torch.max(self.priorities).item()\n",
    "        self.priorities[self.i] = max_priority if max_priority > 0 else self.initial_max_priority\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "        self.i = (self.i + 1) % EXPERIENCE_CAPACITY\n",
    "\n",
    "    def update_beta(self):\n",
    "        self.beta_i += 1\n",
    "        multiplier = min(self.beta_i, self.beta_i_at_max)/self.beta_i_at_max\n",
    "        self.beta = multiplier/(1-self.initial_beta) + self.initial_beta\n",
    "        \n",
    "    def update_priorities(self, losses, indices):\n",
    "        with torch.no_grad():\n",
    "            self.priorities[indices] = losses + 1e-4\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        scaled_priorities = torch.pow(self.priorities, self.alpha)\n",
    "        probabilities = scaled_priorities/torch.sum(scaled_priorities)\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False, p=probabilities[:self.size].numpy())\n",
    "\n",
    "        samples = self.transitions[indices]\n",
    "        weights = torch.pow(probabilities[indices], -self.beta)\n",
    "        normalized_weights = weights/torch.max(weights) \n",
    "        return ImportanceWeighedSample(self, samples, normalized_weights, indices, self.i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Networks, loss fn, optimizer\n",
    "q = DuelQ(env.action_space.n)\n",
    "q_ = DuelQ(env.action_space.n)\n",
    "transfer_state(q, q_)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(q.parameters(), lr=0.005)\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# Prioritised Experience Replay\n",
    "isb = ImportanceSamplingBuffer(1, 0.01)\n",
    "\n",
    "# List of possible actions\n",
    "action_values = [torch.tensor([i], dtype=torch.float32) for i in range(env.action_space.n)]\n",
    "\n",
    "# Replay memory counter, counter for target net update\n",
    "next_i = 0\n",
    "c = 0\n",
    "\n",
    "# Train, test, visualise\n",
    "for i_episode in range(EPISODE_COUNT):\n",
    "    observation = torch.Tensor(env.reset())\n",
    "    eps = max(0.01, ((EPISODE_COUNT - i_episode) / EPISODE_COUNT) * 0.99)\n",
    "    for t in range(EPISODE_LENGTH):\n",
    "\n",
    "        # With probability epsilon select a random action a_t\n",
    "        # otherwise select a_t = argmax_a Q(x_t, a, net parameters)\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = duel_argmax_action(q, observation)\n",
    "\n",
    "        # Execute action a_t in emulator and observe reward r_t and observation x_t+1\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        new_observation = torch.Tensor(new_observation)\n",
    "        \n",
    "        # Store transition (a_t, x_t, r_t, x_t+1, d)\n",
    "        transition_block = empty_transition_block()\n",
    "        transition_block[0][0] = action\n",
    "        transition_block[0][1:OBS_SIZE + 1] = observation\n",
    "        transition_block[0][OBS_SIZE + 1: 2 * OBS_SIZE + 1] = new_observation\n",
    "        transition_block[0][2 * OBS_SIZE + 1] = reward\n",
    "        transition_block[0][2 * OBS_SIZE + 2] = 1 if (done or t == EPISODE_LENGTH - 1) else 0\n",
    "        isb.push(transition_block)\n",
    "        observation = new_observation\n",
    "        next_i += 1\n",
    "        c += 1\n",
    "\n",
    "        # If enough memory stored\n",
    "        if next_i > 500:\n",
    "\n",
    "            # Sample random minibatch of transitions (x_j, a_j, r_j, x_j+1, d_j)\n",
    "            sample = isb.sample(MINIBATCH_SIZE)\n",
    "            batch = sample.batch\n",
    "            # Set targets\n",
    "            rs = batch[:, 2 * OBS_SIZE + 1].view(MINIBATCH_SIZE, 1)\n",
    "            ds = batch[:, 2 * OBS_SIZE + 2].view(MINIBATCH_SIZE, 1)\n",
    "        \n",
    "            # # Choose from :\n",
    "            # # # qs = duel_dqn_target(q_, batch) for Duelling DQN\n",
    "            # # # qs = duel_double_dqn_target(q_, q, batch) for Double Duelling DQN\n",
    "            qs = duel_double_dqn_target(q_, q, batch)\n",
    "            ys = rs + (1 - ds) * gamma * qs\n",
    "            \n",
    "            # Perform a gradient descent step on (y_j - Q(x_j, a_j))^2\n",
    "            optimizer.zero_grad()\n",
    "            outputs = q.forward(batch[:, 1:OBS_SIZE + 1]).gather(1, batch[:, 0].unsqueeze(1).long())\n",
    "            losses = torch.pow((outputs - ys), 2) * sample.weights.unsqueeze(1)\n",
    "\n",
    "            loss = losses.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update priorities and beta\n",
    "            sample.update(losses.squeeze(1))\n",
    "            \n",
    "            # Update target network q_\n",
    "            if USE_SOFT_TARGET_UPDATE:\n",
    "                polyak(q, q_, 0.9)\n",
    "            elif c > TARGET_NET_REFRESH_RATE:\n",
    "                transfer_state(q, q_)\n",
    "                c = 0\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    if i_episode % 10 == 0 and next_i > 500:\n",
    "        all_rewards = 0\n",
    "        count = 100\n",
    "        for j in range(count):\n",
    "            observation = torch.Tensor(env.reset())\n",
    "            rewards = 0\n",
    "            for t in range(EPISODE_LENGTH):\n",
    "                # Uncomment for rendering:\n",
    "                # env.render()\n",
    "\n",
    "                action = duel_argmax_action(q, observation)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                rewards += reward\n",
    "                observation = torch.Tensor(observation)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards += rewards / count\n",
    "        print(\"trained using data from %d episodes, current avg test reward: %d, current beta: %f\" % (i_episode, all_rewards, isb.beta))\n",
    "        if all_rewards > 195:\n",
    "            torch.save(q.state_dict(), \"ISDuelDQNCartpole\")\n",
    "            print(\"Cartpole solved, saved net\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
