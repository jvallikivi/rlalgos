{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG in the Pendulum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "\n",
    "EPISODE_COUNT = 10000\n",
    "EPISODE_LENGTH = 200\n",
    "OBS_SIZE = 3\n",
    "MINIBATCH_SIZE = 16\n",
    "EXPERIENCE_CAPACITY = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(OBS_SIZE + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, low, high):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(OBS_SIZE, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.action_scale = (high - low) / 2\n",
    "        self.action_avg = (high + low) / 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x*self.action_scale + self.action_avg\n",
    "\n",
    "       \n",
    "def transfer_state(src, dst):\n",
    "    dst.load_state_dict(src.state_dict())  \n",
    "    \n",
    "\n",
    "def empty_transition_block():\n",
    "    return torch.zeros(1, OBS_SIZE * 2 + 3)\n",
    "\n",
    "\n",
    "def decreasing_uniform_noise(i):\n",
    "    max_mul = 4\n",
    "    min_mul = 0.5\n",
    "    i_at_min = int(EPISODE_COUNT / 2)\n",
    "    mul = ((i_at_min - min(i, i_at_min))/i_at_min)*(max_mul - min_mul) + min_mul\n",
    "    return (random.random() - 0.5)*mul\n",
    "\n",
    "\n",
    "def clip_with_noise(value, i, low, high):\n",
    "    return torch.clamp(value + decreasing_uniform_noise(i), low, high)\n",
    "\n",
    "\n",
    "def polyak(src, dst, p):\n",
    "    for src_p, dst_p in zip(src.parameters(), dst.parameters()):\n",
    "        dst_p.data.copy_(p*dst_p.data + (1-p)*src_p.data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "low = env.action_space.low.item()\n",
    "high = env.action_space.high.item()\n",
    "\n",
    "# Networks, loss fn, optimizer\n",
    "q = Critic()\n",
    "q_ = Critic()\n",
    "transfer_state(q, q_)\n",
    "\n",
    "a = Actor(low, high)\n",
    "a_ = Actor(low, high)\n",
    "transfer_state(a, a_)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "q_optimizer = optim.Adam(q.parameters(), lr=0.005)\n",
    "a_optimizer = optim.Adam(a.parameters(), lr=0.005)\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# Replay memory\n",
    "transitions = deque(maxlen=EXPERIENCE_CAPACITY)\n",
    "\n",
    "# Replay memory counter, counter for target net update, polyak average multiplier\n",
    "next_i = 0\n",
    "c = 0\n",
    "p = 0.99\n",
    "\n",
    "# Train, test, visualise\n",
    "for i_episode in range(EPISODE_COUNT):\n",
    "    observation = torch.Tensor(env.reset())\n",
    "    for t in range(EPISODE_LENGTH):\n",
    "        # Selection action a_t from the actor network, add noise and clip\n",
    "        with torch.no_grad():\n",
    "            action = clip_with_noise(a.forward(observation), i_episode, low, high)\n",
    "\n",
    "        # Execute action a_t in emulator and observe reward r_t and observation x_t+1\n",
    "        new_observation, reward, done, info = env.step(action.numpy())\n",
    "        new_observation = torch.Tensor(new_observation)\n",
    "\n",
    "        # Store transition (x_t, a_t, r_t, x_t+1, d)\n",
    "        transition_block = empty_transition_block()\n",
    "        transition_block[0][0] = action[0]\n",
    "        transition_block[0][1:OBS_SIZE + 1] = observation\n",
    "        transition_block[0][OBS_SIZE + 1: 2 * OBS_SIZE + 1] = new_observation\n",
    "        transition_block[0][2 * OBS_SIZE + 1] = reward\n",
    "        transition_block[0][2 * OBS_SIZE + 2] = 0 if (done or t == EPISODE_LENGTH - 1) else 1\n",
    "        transitions.append(transition_block)\n",
    "        observation = new_observation\n",
    "        next_i += 1\n",
    "        c += 1\n",
    "\n",
    "        # If enough memory stored\n",
    "        if next_i > 500:\n",
    "            # Sample random minibatch of transitions (x_j, a_j, r_j, x_j+1)\n",
    "            batch = torch.cat(random.sample(transitions, MINIBATCH_SIZE), 0)\n",
    "\n",
    "            # Set targets\n",
    "            rs = batch[:, 2 * OBS_SIZE + 1].view(MINIBATCH_SIZE, 1)\n",
    "            ds = batch[:, 2 * OBS_SIZE + 2].view(MINIBATCH_SIZE, 1)\n",
    "            next_states = batch[:, OBS_SIZE + 1: 2 * OBS_SIZE + 1]\n",
    "            actions = a_.forward(next_states)\n",
    "            ys = rs + gamma * q_.forward(torch.cat((actions, next_states), 1))\n",
    "\n",
    "            # Perform a gradient descent step for critic on (y_j - Q(x_j, a_j))^2\n",
    "            outputs = q.forward(batch[:, :OBS_SIZE + 1])\n",
    "            loss = criterion(outputs, ys)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            utils.clip_grad_norm_(q.parameters(), 5)  # Gradients tend to explode otherwise\n",
    "            q_optimizer.step()\n",
    "\n",
    "            # Perform a gradient ascent step for actor based on gradient of Q w.r.t action\n",
    "            a_optimizer.zero_grad()\n",
    "            q_optimizer.zero_grad()\n",
    "\n",
    "            actions = a.forward(batch[:, 1:OBS_SIZE + 1])\n",
    "            \n",
    "            loss = -1*(q.forward(torch.cat((actions, batch[:, 1:OBS_SIZE + 1]), 1))).sum()\n",
    "            loss.backward()   \n",
    "            a_optimizer.step()\n",
    "\n",
    "            # Update target network weights\n",
    "            polyak(q, q_, p)\n",
    "            polyak(a, a_, p)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if i_episode % 10 == 0 and next_i > 500:\n",
    "        allrewards = 0\n",
    "        count = 100\n",
    "        with torch.no_grad():\n",
    "            for j in range(count):\n",
    "                observation = torch.Tensor(env.reset())\n",
    "                rewards = 0\n",
    "                for t in range(EPISODE_LENGTH):\n",
    "                    # Uncomment for rendering:\n",
    "                    # env.render()\n",
    "                    \n",
    "                    action = a.forward(observation)\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    rewards += reward\n",
    "                    observation = torch.Tensor(observation)\n",
    "                    if done:\n",
    "                        break\n",
    "                allrewards += rewards/count\n",
    "        print(\"trained from %d episodes, current avg test reward: %d\" % (i_episode, allrewards))\n",
    "        if allrewards > -300:\n",
    "            torch.save(a.state_dict(), \"DDPGPendulum\")\n",
    "            print(\"Pendulum solved, saved actor\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
